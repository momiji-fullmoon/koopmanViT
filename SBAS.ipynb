{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クープマン作用素で地盤変動の時系列予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クープマンラボのチュートリアルデータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クープマンラボの中身を抽出して実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 畳み込み処理の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# The structure of Auto-Encoder\n",
    "class encoder_mlp(nn.Module):\n",
    "    def __init__(self, t_len, op_size):\n",
    "        super(encoder_mlp, self).__init__()\n",
    "        self.layer = nn.Linear(t_len, op_size)\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "class decoder_mlp(nn.Module):\n",
    "    def __init__(self, t_len, op_size):\n",
    "        super(decoder_mlp, self).__init__()\n",
    "        self.layer = nn.Linear(op_size, t_len)\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "class encoder_conv1d(nn.Module):\n",
    "    def __init__(self, t_len, op_size):\n",
    "        super(encoder_conv1d, self).__init__()\n",
    "        self.layer = nn.Conv1d(t_len, op_size,1)\n",
    "    def forward(self, x):\n",
    "        x = x.permute([0,2,1])\n",
    "        x = self.layer(x)\n",
    "        x = x.permute([0,2,1])\n",
    "        return x\n",
    "\n",
    "class decoder_conv1d(nn.Module):\n",
    "    def __init__(self, t_len, op_size):\n",
    "        super(decoder_conv1d, self).__init__()\n",
    "        self.layer = nn.Conv1d(op_size, t_len,1)\n",
    "    def forward(self, x):\n",
    "        x = x.permute([0,2,1])\n",
    "        x = self.layer(x)\n",
    "        x = x.permute([0,2,1])\n",
    "        return x\n",
    "\n",
    "class encoder_conv2d(nn.Module):\n",
    "    def __init__(self, t_len, op_size):\n",
    "        super(encoder_conv2d, self).__init__()\n",
    "        self.layer = nn.Conv2d(t_len, op_size,1)\n",
    "    def forward(self, x):\n",
    "        x = x.permute([0,3,1,2])\n",
    "        x = self.layer(x)\n",
    "        x = x.permute([0,2,3,1])\n",
    "        return x\n",
    "\n",
    "class decoder_conv2d(nn.Module):\n",
    "    def __init__(self, t_len, op_size):\n",
    "        super(decoder_conv2d, self).__init__()\n",
    "        self.layer = nn.Conv2d(op_size, t_len,1)\n",
    "    def forward(self, x):\n",
    "        x = x.permute([0,3,1,2])\n",
    "        x = self.layer(x)\n",
    "        x = x.permute([0,2,3,1])\n",
    "        return x\n",
    "\n",
    "# Koopman 1D structure\n",
    "class Koopman_Operator1D(nn.Module):\n",
    "    def __init__(self, op_size, modes_x = 16):\n",
    "        super(Koopman_Operator1D, self).__init__()\n",
    "        self.op_size = op_size\n",
    "        self.scale = (1 / (op_size * op_size))\n",
    "        self.modes_x = modes_x\n",
    "        self.koopman_matrix = nn.Parameter(self.scale * torch.rand(op_size, op_size, self.modes_x, dtype=torch.cfloat))\n",
    "    # Complex multiplication\n",
    "    def time_marching(self, input, weights):\n",
    "        # (batch, t, x), (t, t+1, x) -> (batch, t+1, x)\n",
    "        return torch.einsum(\"btx,tfx->bfx\", input, weights)\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        # Fourier Transform\n",
    "        x_ft = torch.fft.rfft(x)\n",
    "        # Koopman Operator Time Marching\n",
    "        out_ft = torch.zeros(x_ft.shape, dtype=torch.cfloat, device = x.device)\n",
    "        out_ft[:, :, :self.modes_x] = self.time_marching(x_ft[:, :, :self.modes_x], self.koopman_matrix)\n",
    "        #Inverse Fourier Transform\n",
    "        x = torch.fft.irfft(out_ft, n=x.size(-1))\n",
    "        return x\n",
    "\n",
    "class KNO1d(nn.Module):\n",
    "    def __init__(self, encoder, decoder, op_size, modes_x = 16, decompose = 4, linear_type = True, normalization = False):\n",
    "        super(KNO1d, self).__init__()\n",
    "        # Parameter\n",
    "        self.op_size = op_size\n",
    "        self.decompose = decompose\n",
    "        # Layer Structure\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "        self.koopman_layer = Koopman_Operator1D(self.op_size, modes_x = modes_x)\n",
    "        self.w0 = nn.Conv1d(op_size, op_size, 1)\n",
    "        self.linear_type = linear_type # If this variable is False, activate function will be worked after Koopman Matrix\n",
    "        self.normalization = normalization\n",
    "        if self.normalization:\n",
    "            self.norm_layer = torch.nn.BatchNorm2d(op_size)\n",
    "    def forward(self, x):\n",
    "        # Reconstruct\n",
    "        x_reconstruct = self.enc(x)\n",
    "        x_reconstruct = torch.tanh(x_reconstruct)\n",
    "        x_reconstruct = self.dec(x_reconstruct)\n",
    "        # Predict\n",
    "        x = self.enc(x) # Encoder\n",
    "        x = torch.tanh(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x_w = x\n",
    "        for i in range(self.decompose):\n",
    "            x1 = self.koopman_layer(x) # Koopman Operator\n",
    "            if self.linear_type:\n",
    "                x = x + x1\n",
    "            else:\n",
    "                x = torch.tanh(x + x1)\n",
    "        if self.normalization:\n",
    "            x = torch.tanh(self.norm_layer(self.w0(x_w)) + x)\n",
    "        else:\n",
    "            x = torch.tanh(self.w0(x_w) + x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.dec(x) # Decoder\n",
    "        return x, x_reconstruct\n",
    "\n",
    "# Koopman 2D structure\n",
    "class Koopman_Operator2D(nn.Module):\n",
    "    def __init__(self, op_size, modes_x, modes_y):\n",
    "        super(Koopman_Operator2D, self).__init__()\n",
    "        self.op_size = op_size\n",
    "        self.scale = (1 / (op_size * op_size))\n",
    "        self.modes_x = modes_x\n",
    "        self.modes_y = modes_y\n",
    "        self.koopman_matrix = nn.Parameter(self.scale * torch.rand(op_size, op_size, self.modes_x, self.modes_y, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def time_marching(self, input, weights):\n",
    "        # (batch, t, x,y ), (t, t+1, x,y) -> (batch, t+1, x,y)\n",
    "        return torch.einsum(\"btxy,tfxy->bfxy\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        # Fourier Transform\n",
    "        x_ft = torch.fft.rfft2(x)\n",
    "        # Koopman Operator Time Marching\n",
    "        out_ft = torch.zeros(x_ft.shape, dtype=torch.cfloat, device = x.device)\n",
    "        out_ft[:, :, :self.modes_x, :self.modes_y] = self.time_marching(x_ft[:, :, :self.modes_x, :self.modes_y], self.koopman_matrix)\n",
    "        out_ft[:, :, -self.modes_x:, :self.modes_y] = self.time_marching(x_ft[:, :, -self.modes_x:, :self.modes_y], self.koopman_matrix)\n",
    "        #Inverse Fourier Transform\n",
    "        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n",
    "        return x\n",
    "\n",
    "class KNO2d(nn.Module):\n",
    "    def __init__(self, encoder, decoder, op_size, modes_x = 10, modes_y = 10, decompose = 6, linear_type = True, normalization = False):\n",
    "        super(KNO2d, self).__init__()\n",
    "        # Parameter\n",
    "        self.op_size = op_size\n",
    "        self.decompose = decompose\n",
    "        self.modes_x = modes_x\n",
    "        self.modes_y = modes_y\n",
    "        # Layer Structure\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "        self.koopman_layer = Koopman_Operator2D(self.op_size, self.modes_x, self.modes_y)\n",
    "        self.w0 = nn.Conv2d(op_size, op_size, 1)\n",
    "        self.linear_type = linear_type # If this variable is False, activate function will be worked after Koopman Matrix\n",
    "        self.normalization = normalization\n",
    "        if self.normalization:\n",
    "            self.norm_layer = torch.nn.BatchNorm2d(op_size)\n",
    "    def forward(self, x):\n",
    "        # Reconstruct\n",
    "        x_reconstruct = self.enc(x)\n",
    "        x_reconstruct = torch.tanh(x_reconstruct)\n",
    "        x_reconstruct = self.dec(x_reconstruct)\n",
    "        # x_reconstructは入力データを再構成した結果\n",
    "        # Predict\n",
    "        x = self.enc(x) # Encoder\n",
    "        x = torch.tanh(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x_w = x\n",
    "        for i in range(self.decompose):\n",
    "            x1 = self.koopman_layer(x) # Koopman Operator\n",
    "            if self.linear_type:\n",
    "                x = x + x1\n",
    "            else:\n",
    "                x = torch.tanh(x + x1)\n",
    "        if self.normalization:\n",
    "            x = torch.tanh(self.norm_layer(self.w0(x_w)) + x)\n",
    "        else:\n",
    "            x = torch.tanh(self.w0(x_w) + x)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.dec(x) # Decoder\n",
    "        return x, x_reconstruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最適化の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import List, Optional\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "# print the number of parameters\n",
    "def count_params(model):\n",
    "    c = 0\n",
    "    for p in list(model.parameters()):\n",
    "        c += reduce(operator.mul,\n",
    "                    list(p.size()+(2,) if p.is_complex() else p.size()))\n",
    "    return c\n",
    "\n",
    "\n",
    "def adam(params: List[Tensor],\n",
    "         grads: List[Tensor],\n",
    "         exp_avgs: List[Tensor],\n",
    "         exp_avg_sqs: List[Tensor],\n",
    "         max_exp_avg_sqs: List[Tensor],\n",
    "         state_steps: List[int],\n",
    "         *,\n",
    "         amsgrad: bool,\n",
    "         beta1: float,\n",
    "         beta2: float,\n",
    "         lr: float,\n",
    "         weight_decay: float,\n",
    "         eps: float):\n",
    "    r\"\"\"Functional API that performs Adam algorithm computation.\n",
    "    See :class:`~torch.optim.Adam` for details.\n",
    "    \"\"\"\n",
    "\n",
    "    for i, param in enumerate(params):\n",
    "\n",
    "        grad = grads[i]\n",
    "        exp_avg = exp_avgs[i]\n",
    "        exp_avg_sq = exp_avg_sqs[i]\n",
    "        step = state_steps[i]\n",
    "\n",
    "        bias_correction1 = 1 - beta1 ** step\n",
    "        bias_correction2 = 1 - beta2 ** step\n",
    "\n",
    "        if weight_decay != 0:\n",
    "            grad = grad.add(param, alpha=weight_decay)\n",
    "\n",
    "        # Decay the first and second moment running average coefficient\n",
    "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "        exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)\n",
    "        if amsgrad:\n",
    "            # Maintains the maximum of all 2nd moment running avg. till now\n",
    "            torch.maximum(max_exp_avg_sqs[i], exp_avg_sq, out=max_exp_avg_sqs[i])\n",
    "            # Use the max. for normalizing running avg. of gradient\n",
    "            denom = (max_exp_avg_sqs[i].sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
    "        else:\n",
    "            denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
    "\n",
    "        step_size = lr / bias_correction1\n",
    "\n",
    "        param.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    r\"\"\"Implements Adam algorithm.\n",
    "    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n",
    "    The implementation of the L2 penalty follows changes proposed in\n",
    "    `Decoupled Weight Decay Regularization`_.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
    "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
    "            (default: False)\n",
    "    .. _Adam\\: A Method for Stochastic Optimization:\n",
    "        https://arxiv.org/abs/1412.6980\n",
    "    .. _Decoupled Weight Decay Regularization:\n",
    "        https://arxiv.org/abs/1711.05101\n",
    "    .. _On the Convergence of Adam and Beyond:\n",
    "        https://openreview.net/forum?id=ryQu7f-RZ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        if not 0.0 <= weight_decay:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(Adam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adam, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Args:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            grads = []\n",
    "            exp_avgs = []\n",
    "            exp_avg_sqs = []\n",
    "            max_exp_avg_sqs = []\n",
    "            state_steps = []\n",
    "            beta1, beta2 = group['betas']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    if p.grad.is_sparse:\n",
    "                        raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                    grads.append(p.grad)\n",
    "\n",
    "                    state = self.state[p]\n",
    "                    # Lazy state initialization\n",
    "                    if len(state) == 0:\n",
    "                        state['step'] = 0\n",
    "                        # Exponential moving average of gradient values\n",
    "                        state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        # Exponential moving average of squared gradient values\n",
    "                        state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        if group['amsgrad']:\n",
    "                            # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                            state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                    exp_avgs.append(state['exp_avg'])\n",
    "                    exp_avg_sqs.append(state['exp_avg_sq'])\n",
    "\n",
    "                    if group['amsgrad']:\n",
    "                        max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n",
    "\n",
    "                    # update the steps for each param group update\n",
    "                    state['step'] += 1\n",
    "                    # record the step after step update\n",
    "                    state_steps.append(state['step'])\n",
    "\n",
    "            adam(params_with_grad,\n",
    "                 grads,\n",
    "                 exp_avgs,\n",
    "                 exp_avg_sqs,\n",
    "                 max_exp_avg_sqs,\n",
    "                 state_steps,\n",
    "                 amsgrad=group['amsgrad'],\n",
    "                 beta1=beta1,\n",
    "                 beta2=beta2,\n",
    "                 lr=group['lr'],\n",
    "                 weight_decay=group['weight_decay'],\n",
    "                 eps=group['eps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViTの中身"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from copy import Error, deepcopy\n",
    "from re import S\n",
    "from numpy.lib.arraypad import pad\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "import torch.fft\n",
    "from torch.nn.modules.container import Sequential\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=(224, 224), patch_size=(16, 16), in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "# Use Fourier-Transformer structure to approximate linear Koopman Operator\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads=8,\n",
    "        qkv_bias=False,\n",
    "        qk_scale=None,\n",
    "        attn_drop=0.0,\n",
    "        proj_drop=0.0,\n",
    "        input_size=(4, 14, 14),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim**-0.5\n",
    "\n",
    "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        assert attn_drop == 0.0  # do not use\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.input_size = input_size\n",
    "        assert input_size[1] == input_size[2]\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        q = (\n",
    "            self.q(x)\n",
    "            .reshape(B, N, self.num_heads, C // self.num_heads)\n",
    "            .permute(0, 2, 1, 3)\n",
    "        )\n",
    "        k = (\n",
    "            self.k(x)\n",
    "            .reshape(B, N, self.num_heads, C // self.num_heads)\n",
    "            .permute(0, 2, 1, 3)\n",
    "        )\n",
    "        v = (\n",
    "            self.v(x)\n",
    "            .reshape(B, N, self.num_heads, C // self.num_heads)\n",
    "            .permute(0, 2, 1, 3)\n",
    "        )\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        x = x.view(B, -1, C)\n",
    "        return x\n",
    "\n",
    "\n",
    "class At_Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Block in Fourier Domain\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim=768,\n",
    "        num_heads=8,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=False,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        attn_func=Attention,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = attn_func(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ft = torch.fft.fft(x, dim=-1, norm=\"ortho\")\n",
    "        x_r = x_ft.real\n",
    "        x_i = x_ft.imag\n",
    "        # Calculate Real Part\n",
    "        x_r = x_r + self.drop_path(self.attn(self.norm1(x_r)))\n",
    "        x_r = x_r + self.drop_path(self.mlp(self.norm2(x_r)))\n",
    "        # Calculate Imaginary Part\n",
    "        x_i = x_i + self.drop_path(self.attn(self.norm1(x_i)))\n",
    "        x_i = x_i + self.drop_path(self.mlp(self.norm2(x_i)))\n",
    "        # Merge\n",
    "        x_ft.real = x_r\n",
    "        x_ft.imag = x_i\n",
    "        x = torch.fft.ifft(x_ft, dim=-1, norm=\"ortho\")\n",
    "        return x\n",
    "\n",
    "# Use linear AFNO1D structure to approximate linear Koopman Operator\n",
    "class AFNO1D(nn.Module):\n",
    "    def __init__(self, hidden_size, num_blocks=8, sparsity_threshold=0.01, hard_thresholding_fraction=1, hidden_size_factor=1):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_blocks == 0, f\"hidden_size {hidden_size} should be divisble by num_blocks {num_blocks}\"\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sparsity_threshold = sparsity_threshold\n",
    "        self.num_blocks = num_blocks\n",
    "        self.block_size = self.hidden_size // self.num_blocks\n",
    "        self.hard_thresholding_fraction = hard_thresholding_fraction\n",
    "        self.hidden_size_factor = hidden_size_factor\n",
    "        self.scale = 0.02\n",
    "\n",
    "        self.w1 = nn.Parameter(self.scale * torch.randn(2, self.num_blocks, self.block_size, self.block_size * self.hidden_size_factor))\n",
    "        self.b1 = nn.Parameter(self.scale * torch.randn(2, self.num_blocks, self.block_size * self.hidden_size_factor))\n",
    "        self.w2 = nn.Parameter(self.scale * torch.randn(2, self.num_blocks, self.block_size * self.hidden_size_factor, self.block_size))\n",
    "        self.b2 = nn.Parameter(self.scale * torch.randn(2, self.num_blocks, self.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        bias = x\n",
    "\n",
    "        dtype = x.dtype\n",
    "        x = x.float()\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        x = torch.fft.rfft(x, dim=1, norm=\"ortho\")\n",
    "        x = x.reshape(B, N // 2 + 1, self.num_blocks, self.block_size)\n",
    "\n",
    "        o1_real = torch.zeros([B, N // 2 + 1, self.num_blocks, self.block_size * self.hidden_size_factor], device=x.device)\n",
    "        o1_imag = torch.zeros([B, N // 2 + 1, self.num_blocks, self.block_size * self.hidden_size_factor], device=x.device)\n",
    "        o2_real = torch.zeros(x.shape, device=x.device)\n",
    "        o2_imag = torch.zeros(x.shape, device=x.device)\n",
    "\n",
    "        total_modes = N // 2 + 1\n",
    "        kept_modes = int(total_modes * self.hard_thresholding_fraction)\n",
    "\n",
    "        o1_real[:, :kept_modes] = F.relu(\n",
    "            torch.einsum('...bi,bio->...bo', x[:, :kept_modes].real, self.w1[0]) - \\\n",
    "            torch.einsum('...bi,bio->...bo', x[:, :kept_modes].imag, self.w1[1]) + \\\n",
    "            self.b1[0]\n",
    "        )\n",
    "\n",
    "        o1_imag[:, :kept_modes] = F.relu(\n",
    "            torch.einsum('...bi,bio->...bo', x[:, :kept_modes].imag, self.w1[0]) + \\\n",
    "            torch.einsum('...bi,bio->...bo', x[:, :kept_modes].real, self.w1[1]) + \\\n",
    "            self.b1[1]\n",
    "        )\n",
    "\n",
    "        o2_real[:, :kept_modes] = (\n",
    "            torch.einsum('...bi,bio->...bo', o1_real[:, :kept_modes], self.w2[0]) - \\\n",
    "            torch.einsum('...bi,bio->...bo', o1_imag[:, :kept_modes], self.w2[1]) + \\\n",
    "            self.b2[0]\n",
    "        )\n",
    "\n",
    "        o2_imag[:, :kept_modes] = (\n",
    "            torch.einsum('...bi,bio->...bo', o1_imag[:, :kept_modes], self.w2[0]) + \\\n",
    "            torch.einsum('...bi,bio->...bo', o1_real[:, :kept_modes], self.w2[1]) + \\\n",
    "            self.b2[1]\n",
    "        )\n",
    "\n",
    "        x = torch.stack([o2_real, o2_imag], dim=-1)\n",
    "        x = F.softshrink(x, lambd=self.sparsity_threshold)\n",
    "        x = torch.view_as_complex(x)\n",
    "        x = x.reshape(B, N // 2 + 1, C)\n",
    "        x = torch.fft.irfft(x, n=N, dim=1, norm=\"ortho\")\n",
    "        x = x.type(dtype)\n",
    "        return x + bias\n",
    "\n",
    "class Af_Block(nn.Module):\n",
    "    \"\"\"\n",
    "    AdaptiveFNO Block\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            mlp_ratio=4.,\n",
    "            drop=0.,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            double_skip=True,\n",
    "            num_blocks=8,\n",
    "            sparsity_threshold=0.01,\n",
    "            hard_thresholding_fraction=1.0,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.filter = AFNO1D(dim, num_blocks, sparsity_threshold, hard_thresholding_fraction)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        #self.drop_path = nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        self.double_skip = double_skip\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.filter(x)\n",
    "\n",
    "        if self.double_skip:\n",
    "            x = x + residual\n",
    "            residual = x\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.drop_path(x)\n",
    "        x = x + residual\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size=(720, 1440),\n",
    "            patch_size=(8, 8),\n",
    "            in_chans=20,\n",
    "            out_chans=20,\n",
    "            embed_dim=768,\n",
    "            encoder_depth = 2,\n",
    "            depth=10,\n",
    "            mlp_ratio=4.,\n",
    "            drop_rate=0.,\n",
    "            drop_path_rate=0.,\n",
    "            num_blocks=16,\n",
    "            sparsity_threshold=0.01,\n",
    "            hard_thresholding_fraction=1.0,\n",
    "            settings = \"Conv2d\",\n",
    "            encoder_network = False\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.encoder_network = encoder_network\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        \n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_blocks = num_blocks \n",
    "        self.depth = depth\n",
    "        self.settings = settings\n",
    "        norm_layer = partial(nn.LayerNorm, eps=1e-6)\n",
    "\n",
    "        self.patch_embed = PatchEmbed(img_size=self.img_size, patch_size=self.patch_size, in_chans=self.in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, self.depth)]\n",
    "        self.dpr = dpr\n",
    "        \n",
    "        self.h = img_size[0] // self.patch_size[0]\n",
    "        self.w = img_size[1] // self.patch_size[1]\n",
    "        \n",
    "        \n",
    "        # Encoder Settings\n",
    "        self.encoder_depth = encoder_depth\n",
    "        # There are two options. Af_Block represents using the AdaptiveFNO blocks, and At_Block represents using the Fourier-Transformer blocks.\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            Af_Block(dim=embed_dim, mlp_ratio=mlp_ratio, drop=drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "            num_blocks=self.num_blocks, sparsity_threshold=sparsity_threshold, hard_thresholding_fraction=hard_thresholding_fraction)\n",
    "            for i in range(encoder_depth)])\n",
    "        \n",
    "        # Koopman Layers\n",
    "        self.core_blocks = nn.ModuleList([\n",
    "            Af_Block(dim=embed_dim, mlp_ratio=mlp_ratio, drop=drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "            num_blocks=self.num_blocks, sparsity_threshold=sparsity_threshold, hard_thresholding_fraction=hard_thresholding_fraction) \n",
    "        for i in range(self.depth)])\n",
    "\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        \n",
    "        # High-frequency component\n",
    "        self.w0 = nn.Conv1d(embed_dim, embed_dim, 1) # or user-defined more complicated convolutional structure\n",
    "        \n",
    "        # Decoder Settings\n",
    "        if self.settings == \"MLP\":\n",
    "            self.decoder_pred_mlp = nn.Linear(self.embed_dim, self.out_chans*self.patch_size[0]*self.patch_size[1], bias=False)\n",
    "        elif self.settings == \"Conv2d\":\n",
    "            self.decoder_pred_conv2d = nn.ConvTranspose2d(self.embed_dim, self.out_chans, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        \n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "    \n",
    "    def encoder(self, x):\n",
    "        # Position Encoder\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        # Encoder Network (if reconstruction task is hard, please use more complicated structure)\n",
    "        for blk in self.encoder_blocks:\n",
    "            x = blk(x)\n",
    "            \n",
    "        if self.encoder_network:\n",
    "            x = self.encoder_network(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def decoder(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = x.reshape(B, self.h, self.w, self.embed_dim)\n",
    "        if self.settings == \"MLP\":\n",
    "            x = self.decoder_pred_mlp(x)\n",
    "            x = rearrange(\n",
    "                x,\n",
    "                \"b h w (p1 p2 c_out) -> b c_out (h p1) (w p2)\",\n",
    "                p1=self.patch_size[0],\n",
    "                p2=self.patch_size[1],\n",
    "                h=self.img_size[0] // self.patch_size[0],\n",
    "                w=self.img_size[1] // self.patch_size[1],\n",
    "            )\n",
    "        elif self.settings == \"Conv2d\":\n",
    "            x = rearrange(x, \"B H W C -> B C H W\")\n",
    "            x = self.decoder_pred_conv2d(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        # Reconstruction\n",
    "        x_recons = self.decoder(x)\n",
    "        # Prediction\n",
    "        x_w = self.w0(x.permute(0,2,1)).permute(0,2,1)\n",
    "        for blk in self.core_blocks:\n",
    "            x = blk(x)\n",
    "        x = blk(x)\n",
    "        x = x + x_w\n",
    "        x = self.decoder(x)\n",
    "        return x, x_recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class koopman_vit:\n",
    "    def __init__(self, decoder = \"Conv2d\", depth = 16, resolution=(256, 256), patch_size=(4, 4),\n",
    "            in_chans=1, out_chans=1, embed_dim=768, parallel = False, device = False):\n",
    "        # Model Hyper-parameters\n",
    "        self.decoder = decoder\n",
    "        self.resolution = resolution\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.embed_dim = embed_dim\n",
    "        self.depth = depth\n",
    "        self.num_blocks = 16\n",
    "        # Core Model\n",
    "        self.params = 0\n",
    "        self.kernel = False\n",
    "        # Opt Setting\n",
    "        self.optimizer = False\n",
    "        self.scheduler = False\n",
    "        self.device = device\n",
    "        self.parallel = parallel\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "    def compile(self):\n",
    "        self.kernel = ViT(img_size=self.resolution, patch_size=self.patch_size, in_chans=self.in_chans, out_chans=self.out_chans, num_blocks=self.num_blocks, embed_dim = self.embed_dim, depth=self.depth, settings = self.decoder).to(self.device)\n",
    "        if self.parallel:\n",
    "            self.kernel = torch.nn.DataParallel(self.kernel)\n",
    "        self.params = utils.count_params(self.kernel)\n",
    "        \n",
    "        print(\"Koopman Fourier Vision Transformer has been compiled!\")\n",
    "        print(\"The Model Parameters Number is \",self.params)\n",
    "        \n",
    "    def opt_init(self, opt, lr, step_size, gamma):\n",
    "        if opt == \"Adam\":\n",
    "            self.optimizer = utils.Adam(self.kernel.parameters(), lr= lr, weight_decay=1e-4)\n",
    "        if not step_size == False:\n",
    "            self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=step_size, gamma=gamma)\n",
    "        \n",
    "    def train_multi(self, epochs, trainloader, T_out = 10, evalloader = False):\n",
    "        T_eval = T_out\n",
    "        for ep in range(epochs):\n",
    "            self.kernel.train()\n",
    "            t1 = default_timer()\n",
    "            train_recons_full = 0\n",
    "            train_pred_full = 0\n",
    "            for xx, yy in trainloader:\n",
    "                l_recons = 0\n",
    "                xx = xx.to(self.device) # [batchsize,1,x,y]\n",
    "                # print(xx.size())\n",
    "                yy = yy.to(self.device) # [batchsize,T,x,y]\n",
    "                bs = xx.shape[0]\n",
    "                for t in range(0, T_out):\n",
    "                    y = yy[:, t:t + 1]\n",
    "                    im,im_re = self.kernel(xx)\n",
    "                    # print(im.size())\n",
    "                    # print(im_re.size())\n",
    "                    # print(xx.reshape(bs, -1).size())\n",
    "                    # print(im_re.reshape(bs, -1).size())\n",
    "                    l_recons += self.loss(im_re.reshape(bs, -1), xx.reshape(bs, -1))\n",
    "                    \n",
    "                    if t == 0:\n",
    "                        pred = im[:, -1:]\n",
    "                    else:\n",
    "                        pred = torch.cat((pred, im[:, -1:]), -1)\n",
    "                    \n",
    "                    xx = im\n",
    "                \n",
    "                l_pred = self.loss(pred.reshape(bs, -1), yy.reshape(bs, -1))\n",
    "                loss = 5 * l_pred + 0.5 * l_recons\n",
    "                \n",
    "                train_pred_full += l_pred.item()\n",
    "                train_recons_full += l_recons.item()/T_out\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            train_pred_full = train_pred_full / len(trainloader)\n",
    "            train_recons_full = train_recons_full / len(trainloader)\n",
    "            t2 = default_timer()\n",
    "            test_pred_full = 0\n",
    "            test_recons_full = 0\n",
    "            loc = 0\n",
    "            mse_error = 0\n",
    "            if evalloader:\n",
    "                with torch.no_grad():\n",
    "                    for xx, yy in evalloader:\n",
    "                        loss = 0\n",
    "                        xx = xx.to(self.device)\n",
    "                        yy = yy.to(self.device)\n",
    "\n",
    "                        for t in range(0, T_eval):\n",
    "                            y = yy[:, t:t + 1]\n",
    "                            im, im_re = self.kernel(xx)\n",
    "                            \n",
    "                            l_recons += self.loss(im_re.reshape(bs, -1), xx.reshape(bs, -1))\n",
    "                            \n",
    "                            if t == 0:\n",
    "                                pred = im\n",
    "                            else:\n",
    "                                pred = torch.cat((pred, im), 1)\n",
    "                                \n",
    "                            xx = im\n",
    "                            \n",
    "                        l_pred = self.loss(pred.reshape(bs, -1), yy.reshape(bs, -1))\n",
    "\n",
    "                        test_recons_full += l_recons.item() / T_eval\n",
    "                        test_pred_full += l_pred.item()\n",
    "                        \n",
    "                test_recons_full = test_recons_full / len(evalloader)\n",
    "                test_pred_full = test_pred_full / len(evalloader)\n",
    "            self.scheduler.step()\n",
    "\n",
    "            if evalloader:\n",
    "                if ep == 0:\n",
    "                    print(\"Epoch\",\"Time\",\"[Train Recons MSE]\",\"[Train Pred MSE]\",\"[Eval Recons MSE]\",\"[Eval Pred MSE]\")\n",
    "                print(ep, t2 - t1, train_recons_full, train_pred_full, test_recons_full, test_pred_full)\n",
    "            else:\n",
    "                if ep == 0:\n",
    "                    print(\"Epoch\",\"Time\",\"Train Recons MSE\",\"Train Pred MSE\")\n",
    "                print(ep, t2 - t1, train_recons_full, train_pred_full)\n",
    "    \n",
    "    def test_multi(self, testloader, step = 1, T_out = 5, path = False, is_save = False, is_plot = False):\n",
    "        time_error = torch.zeros([T_out,1])\n",
    "        test_pred_full = 0\n",
    "        test_recons_full = 0\n",
    "        loc = 0\n",
    "        with torch.no_grad():\n",
    "            for xx, yy in testloader:\n",
    "                loss = 0\n",
    "                bs = xx.shape[0]\n",
    "                xx = xx.to(self.device)\n",
    "                yy = yy.to(self.device)\n",
    "                l_recons = 0\n",
    "                for t in range(0, T_out):\n",
    "                    y = yy[:, t:t + 1]\n",
    "                    im, im_re = self.kernel(xx)\n",
    "                    \n",
    "                    \n",
    "                    l_recons += self.loss(im_re.reshape(bs, -1), xx.reshape(bs, -1))\n",
    "                    t_error = self.loss(im, y)\n",
    "                    \n",
    "                    xx = im\n",
    "                    \n",
    "                    if t == 0:\n",
    "                        pred = im\n",
    "                    else:\n",
    "                        pred = torch.cat((pred, im), 1)\n",
    "                    time_error[t] = time_error[t] + t_error.item()\n",
    "    \n",
    "                test_recons_full += l_recons.item() / T_out\n",
    "                l_pred = self.loss(pred.reshape(bs, -1), yy.reshape(bs, -1))\n",
    "                test_pred_full += l_pred.item()\n",
    "\n",
    "                if(loc == 0 & is_save):\n",
    "                    torch.save({\"pred\":pred, \"yy\":yy}, path+ \"pred_yy.pt\")\n",
    "                \n",
    "                if(loc == 0 & is_plot):\n",
    "                    for i in range(T_out):\n",
    "                        plt.subplot(1,3,1)\n",
    "                        plt.title(\"Predict\")\n",
    "                        plt.imshow(pred[0,i].cpu().detach().numpy())\n",
    "                        plt.subplot(1,3,2)\n",
    "                        plt.imshow(yy[0,i].cpu().detach().numpy())\n",
    "                        plt.title(\"Label\")\n",
    "                        plt.subplot(1,3,3)\n",
    "                        plt.imshow(pred[0,i].cpu().detach().numpy()-yy[0,i].cpu().detach().numpy())\n",
    "                        plt.title(\"Error\")\n",
    "                        plt.show()\n",
    "                        plt.savefig(path + \"time_\"+str(i)+\".png\")\n",
    "                        plt.close()\n",
    "\n",
    "                loc = loc + 1\n",
    "        test_pred_full = test_pred_full / loc\n",
    "        test_recons_full = test_recons_full / loc\n",
    "        time_error = time_error / len(testloader)\n",
    "        print(\"Total prediction test mse error is \",test_pred_full)\n",
    "        print(\"Total reconstruction test mse error is \",test_recons_full)\n",
    "        return time_error\n",
    "        \n",
    "        \n",
    "    def train_single(self, epochs, trainloader, evalloader = False):\n",
    "        for ep in range(epochs):\n",
    "            self.kernel.train()\n",
    "            t1 = default_timer()\n",
    "            train_recons_full = 0\n",
    "            train_pred_full = 0\n",
    "            for x, y in trainloader:\n",
    "                l_recons = 0\n",
    "                x = x.to(self.device) # [batchsize,1,64,64]\n",
    "                y = y.to(self.device) # [batchsize,1,64,64]\n",
    "                bs = x.shape[0]\n",
    "                \n",
    "                im,im_re = self.kernel(x)\n",
    "                \n",
    "                l_recons = self.loss(im_re.reshape(bs, -1), x.reshape(bs, -1))\n",
    "                l_pred = self.loss(im.reshape(bs, -1), y.reshape(bs, -1))\n",
    "                \n",
    "                loss = 5 * l_pred + 0.5 * l_recons\n",
    "                \n",
    "                train_pred_full += l_pred.item()\n",
    "                train_recons_full += l_recons.item()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            train_pred_full = train_pred_full / len(trainloader)\n",
    "            train_recons_full = train_recons_full / len(trainloader)\n",
    "            t2 = default_timer()\n",
    "            test_pred_full = 0\n",
    "            test_recons_full = 0\n",
    "            loc = 0\n",
    "            mse_error = 0\n",
    "            if evalloader:\n",
    "                with torch.no_grad():\n",
    "                    for x, y in evalloader:\n",
    "                        loss = 0\n",
    "                        x = x.to(self.device)\n",
    "                        y = y.to(self.device)\n",
    "                        \n",
    "                        im, im_re = self.kernel(x)\n",
    "\n",
    "                        l_recons = self.loss(im_re.reshape(bs, -1), x.reshape(bs, -1))\n",
    "                        l_pred = self.loss(im.reshape(bs, -1), y.reshape(bs, -1))\n",
    "\n",
    "                        test_recons_full += l_recons.item()\n",
    "                        test_pred_full += l_pred.item()\n",
    "                        \n",
    "                test_recons_full = test_recons_full / len(evalloader)\n",
    "                test_pred_full = test_pred_full / len(evalloader)\n",
    "            self.scheduler.step()\n",
    "\n",
    "            if evalloader:\n",
    "                if ep == 0:\n",
    "                    print(\"Epoch\",\"Time\",\"[Train Recons MSE]\",\"[Train Pred MSE]\",\"[Eval Recons MSE]\",\"[Eval Pred MSE]\")\n",
    "                print(ep, t2 - t1, train_recons_full, train_pred_full, test_recons_full, test_pred_full)\n",
    "            else:\n",
    "                if ep == 0:\n",
    "                    print(\"Epoch\",\"Time\",\"Train Recons MSE\",\"Train Pred MSE\")\n",
    "                print(ep, t2 - t1, train_recons_full, train_pred_full)\n",
    "                \n",
    "    def test_single(self, testloader, T_out = 1, path = False, is_save = False, is_plot = False):\n",
    "        time_error = torch.zeros([T_out,1])\n",
    "        test_pred_full = 0\n",
    "        test_recons_full = 0\n",
    "        loc = 0\n",
    "        idx = np.random.randint(0,len(testloader))\n",
    "        with torch.no_grad():\n",
    "            for xx, yy in testloader:\n",
    "                loss = 0\n",
    "                bs = xx.shape[0]\n",
    "                xx = xx.to(self.device)\n",
    "                yy = yy.to(self.device)\n",
    "                l_recons = 0\n",
    "                for t in range(0, T_out):\n",
    "                    y = yy[:, t:t + 1]\n",
    "                    im, im_re = self.kernel(xx)\n",
    "                    \n",
    "                    l_recons += self.loss(im_re.reshape(bs, -1), xx.reshape(bs, -1))\n",
    "                    t_error = self.loss(im, y)\n",
    "                    \n",
    "                    xx = im\n",
    "                    \n",
    "                    if t == 0:\n",
    "                        pred = im\n",
    "                    else:\n",
    "                        pred = torch.cat((pred, im), 1)\n",
    "                    time_error[t] = time_error[t] + t_error.item()\n",
    "    \n",
    "                test_recons_full += l_recons.item() / T_out\n",
    "                l_pred = self.loss(pred.reshape(bs, -1), yy.reshape(bs, -1))\n",
    "                test_pred_full += l_pred.item()\n",
    "\n",
    "                if(loc == 0 & is_save):\n",
    "                    torch.save({\"pred\":pred, \"yy\":yy}, path+ \"pred_yy.pt\")\n",
    "                \n",
    "                if(loc == 0 & is_plot):\n",
    "                    for i in range(T_out):\n",
    "                        plt.subplot(1,3,1)\n",
    "                        plt.title(\"Predict\")\n",
    "                        plt.imshow(pred[0,i].cpu().detach().numpy())\n",
    "                        plt.subplot(1,3,2)\n",
    "                        plt.imshow(yy[0,i].cpu().detach().numpy())\n",
    "                        plt.title(\"Label\")\n",
    "                        plt.subplot(1,3,3)\n",
    "                        plt.imshow(pred[0,i].cpu().detach().numpy()-yy[0,i].cpu().detach().numpy())\n",
    "                        plt.title(\"Error\")\n",
    "                        plt.show()\n",
    "                        plt.savefig(path + \"time_\"+str(i)+\".png\")\n",
    "                        plt.close()\n",
    "                loc = loc + 1\n",
    "\n",
    "        test_pred_full = test_pred_full / len(testloader)\n",
    "        test_recons_full = test_recons_full / len(testloader)\n",
    "        time_error = time_error / len(testloader)\n",
    "        print(\"Total prediction test mse error is \",test_pred_full)\n",
    "        print(\"Total reconstruction test mse error is \",test_recons_full)\n",
    "        \n",
    "        return time_error\n",
    "        \n",
    "    def save(self, path):\n",
    "#        (fpath,_) = os.path.split(path)\n",
    "#        print(fpath, os.path.isfile(fpath))\n",
    "#        if not os.path.isfile(fpath):\n",
    "#            os.makedirs(fpath)\n",
    "        torch.save({\"koopman\":self,\"model\":self.kernel,\"model_params\":self.kernel.state_dict()}, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNの学習クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer\n",
    "# ep = 1000 # Training Epoch\n",
    "# o = 32 # Koopman Operator Size\n",
    "# m = 16 # Modes\n",
    "# r = 8 # Power of Koopman Matrix\n",
    "class koopman:\n",
    "    def __init__(self, backbone = \"KNO2d\", autoencoder = \"Conv2d\", o = 16, m = 16, r = 8, t_in = 1, device = False):\n",
    "        self.backbone = backbone\n",
    "        self.autoencoder = autoencoder\n",
    "        self.operator_size = o\n",
    "        self.modes = m\n",
    "        self.decompose = r\n",
    "        self.device = device\n",
    "        self.t_in = t_in\n",
    "        # Core Model\n",
    "        self.params = 0\n",
    "        self.kernel = False\n",
    "        # Opt Setting\n",
    "        self.optimizer = False\n",
    "        self.scheduler = False\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "    def compile(self):\n",
    "        if self.autoencoder == \"MLP\":\n",
    "            encoder = encoder_mlp(self.t_in, self.operator_size)\n",
    "            decoder = decoder_mlp(self.t_in, self.operator_size)\n",
    "            print(\"The autoencoder type is MLP.\")\n",
    "        elif self.autoencoder == \"Conv1d\":\n",
    "            encoder = encoder_conv1d(self.t_in, self.operator_size)\n",
    "            decoder = decoder_conv1d(self.t_in, self.operator_size)\n",
    "            print(\"The autoencoder type is Conv1d.\")\n",
    "        elif self.autoencoder == \"Conv2d\":\n",
    "            encoder = encoder_conv2d(self.t_in, self.operator_size)\n",
    "            decoder = decoder_conv2d(self.t_in, self.operator_size)\n",
    "            print(\"The autoencoder type is Conv2d.\")\n",
    "        else:\n",
    "#            encoder = kno.encoder_mlp(self.t_in, self.operator_size)\n",
    "#            decoder = kno.decoder_mlp(self.t_in, self.operator_size)\n",
    "#            print(\"The autoencoder type is MLP.\")\n",
    "            print(\"Wrong!\")\n",
    "        if self.backbone == \"KNO1d\":\n",
    "            self.kernel = KNO1d(encoder, decoder, self.operator_size, modes_x = self.modes, decompose = self.decompose).to(self.device)\n",
    "            print(\"KNO1d model is completed.\")\n",
    "        \n",
    "        elif self.backbone == \"KNO2d\":\n",
    "            self.kernel = KNO2d(encoder, decoder, self.operator_size, modes_x = self.modes, modes_y = self.modes,decompose = self.decompose).to(self.device)\n",
    "            print(\"KNO2d model is completed.\")\n",
    "        elif self.backbone == \"DMD\":\n",
    "            self.kernel = KNO2d_DMD(encoder, decoder, self.operator_size, decompose = self.decompose).to(self.device)\n",
    "            print(\"KNO2d_DMD model is completed.\")\n",
    "        elif self.backbone == \"SSM\":\n",
    "            self.kernel = SSM2d(encoder, decoder, self.operator_size, decompose = self.decompose, hidden_size=self.operator_size ).to(self.device)\n",
    "            print(\"SSM2d model is completed.\")\n",
    "        if not self.kernel == False:\n",
    "            self.params = count_params(self.kernel)\n",
    "            print(\"Koopman Model has been compiled!\")\n",
    "            print(\"The Model Parameters Number is \",self.params)\n",
    "    def opt_init(self, opt, lr, step_size, gamma):\n",
    "        if opt == \"Adam\":\n",
    "            self.optimizer = Adam(self.kernel.parameters(), lr= lr, weight_decay=1e-4)\n",
    "        if not step_size == False:\n",
    "            self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    def train_single(self, epochs, trainloader, evalloader = False):\n",
    "        for ep in range(epochs):\n",
    "            # Train\n",
    "            self.kernel.train()\n",
    "            t1 = default_timer()\n",
    "            train_recons_full = 0\n",
    "            train_pred_full = 0\n",
    "            for xx, yy in trainloader:\n",
    "                l_recons = 0\n",
    "                bs = xx.shape[0]\n",
    "                xx = xx.to(self.device)\n",
    "                yy = yy.to(self.device)\n",
    "                pred,im_re = self.kernel(xx)\n",
    "                \n",
    "                l_recons = self.loss(im_re.reshape(bs, -1), xx.reshape(bs, -1))\n",
    "                l_pred = self.loss(pred.reshape(bs, -1), yy.reshape(bs, -1))\n",
    "\n",
    "                train_pred_full += l_pred.item()\n",
    "                train_recons_full += l_recons.item()\n",
    "\n",
    "                loss = 5*l_pred + 0.5*l_recons\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            train_pred_full = train_pred_full / len(trainloader)\n",
    "            train_recons_full = train_recons_full / len(trainloader)\n",
    "            t2 = default_timer()\n",
    "            test_pred_full = 0\n",
    "            test_recons_full = 0\n",
    "            mse_test = 0\n",
    "            # Test\n",
    "            if evalloader:\n",
    "                with torch.no_grad():\n",
    "                    for xx, yy in evalloader:\n",
    "                        bs = xx.shape[0]\n",
    "                        loss = 0\n",
    "                        xx = xx.to(self.device)\n",
    "                        yy = yy.to(self.device)\n",
    "\n",
    "                        pred,im_re = self.kernel(xx)\n",
    "\n",
    "\n",
    "                        l_recons = self.loss(im_re.reshape(bs, -1), xx.reshape(bs, -1))\n",
    "                        l_pred = self.loss(pred.reshape(bs, -1), yy.reshape(bs, -1))\n",
    "\n",
    "\n",
    "                        test_pred_full += l_pred.item()\n",
    "                        test_recons_full += l_recons.item()\n",
    "                        \n",
    "                test_pred_full = test_pred_full/len(evalloader)\n",
    "                test_recons_full = test_recons_full/len(evalloader)\n",
    "                \n",
    "            self.scheduler.step()\n",
    "\n",
    "            if evalloader:\n",
    "                if ep == 0:\n",
    "                    print(\"Epoch\",\"Time\",\"[Train Recons MSE]\",\"[Train Pred MSE]\",\"[Eval Recons MSE]\",\"[Eval Pred MSE]\")\n",
    "                print(ep, t2 - t1, train_recons_full, train_pred_full, test_recons_full, test_pred_full)\n",
    "            else:\n",
    "                if ep == 0:\n",
    "                    print(\"Epoch\",\"Time\",\"Train Recons MSE\",\"Train Pred MSE\")\n",
    "                print(ep, t2 - t1, train_recons_full, train_pred_full)\n",
    "\n",
    "    def test_single(self, testloader):\n",
    "        test_pred_full = 0\n",
    "        test_recons_full = 0\n",
    "        with torch.no_grad():\n",
    "            for xx, yy in testloader:\n",
    "                bs = xx.shape[0]\n",
    "                loss = 0\n",
    "                xx = xx.to(self.device)\n",
    "                yy = yy.to(self.device)\n",
    "\n",
    "                pred,im_re = self.kernel(xx)\n",
    "\n",
    "                l_recons = self.loss(im_re.reshape(bs, -1), xx.reshape(bs, -1))\n",
    "                l_pred = self.loss(pred.reshape(bs, -1), yy.reshape(bs, -1))\n",
    "\n",
    "                test_pred_full += l_pred.item()\n",
    "                test_recons_full += l_recons.item()\n",
    "        test_pred_full = test_pred_full/len(testloader)\n",
    "        test_recons_full = test_recons_full/len(testloader)\n",
    "        print(\"Total prediction test mse error is \",test_pred_full)\n",
    "        print(\"Total reconstruction test mse error is \",test_recons_full)\n",
    "        return test_pred_full\n",
    "\n",
    "\n",
    "    def train(self, epochs, trainloader, step = 1, T_out = 40, T_eval = 80, evalloader = False):\n",
    "        for ep in range(epochs):\n",
    "            self.kernel.train()\n",
    "            t1 = default_timer()\n",
    "            train_recons_full = 0\n",
    "            train_pred_full = 0\n",
    "            for xx, yy in trainloader:\n",
    "                l_recons = 0\n",
    "                xx = xx.to(self.device)\n",
    "                # print(xx.size())\n",
    "                yy = yy.to(self.device)\n",
    "                bs = xx.shape[0]\n",
    "                for t in range(0, T_out):\n",
    "                    y = yy[..., t:t + 1]\n",
    "\n",
    "                    im,im_re = self.kernel(xx)\n",
    "                    # print(im.size()) # 予測\n",
    "                    # print(im_re.size()) # 入力の再構成\n",
    "                    l_recons += self.loss(im_re.reshape(bs, -1), xx.reshape(bs, -1))\n",
    "                    if t == 0:\n",
    "                        pred = im[...,-1:]\n",
    "                    else:\n",
    "                        pred = torch.cat((pred, im[...,-1:]), -1)\n",
    "                    \n",
    "                    xx = torch.cat((xx[..., step:], im[...,-1:]), dim=-1)\n",
    "\n",
    "                \n",
    "                l_pred = self.loss(pred.reshape(bs, -1), yy.reshape(bs, -1))\n",
    "                loss = 5 * l_pred + 0.5 * l_recons\n",
    "                \n",
    "                train_pred_full += l_pred.item()\n",
    "                train_recons_full += l_recons.item()/T_out\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            train_pred_full = train_pred_full / len(trainloader)\n",
    "            train_recons_full = train_recons_full / len(trainloader)\n",
    "            t2 = default_timer()\n",
    "            test_pred_full = 0\n",
    "            test_recons_full = 0\n",
    "            loc = 0\n",
    "            mse_error = 0\n",
    "            if evalloader:\n",
    "                with torch.no_grad():\n",
    "                    for xx, yy in evalloader:\n",
    "                        loss = 0\n",
    "                        xx = xx.to(self.device)\n",
    "                        yy = yy.to(self.device)\n",
    "\n",
    "                        for t in range(0, T_eval):\n",
    "                            y = yy[..., t:t + 1]\n",
    "                            im, im_re = self.kernel(xx)\n",
    "                            l_recons += self.loss(im_re.reshape(bs, -1), xx.reshape(bs, -1))\n",
    "                            if t == 0:\n",
    "                                pred = im[...,-1:]\n",
    "                            else:\n",
    "                                pred = torch.cat((pred, im[...,-1:]), -1)\n",
    "                            xx = torch.cat((xx[..., 1:], im[...,-1:]), dim=-1)\n",
    "                        # print(\"pred: \")\n",
    "                        # print(pred.size())\n",
    "                        # print(\"yy: \")\n",
    "                        # print(yy.size()) \n",
    "                        # l_pred = self.loss(pred.reshape(bs, -1), yy.reshape(bs, -1))\n",
    "\n",
    "                        test_recons_full += l_recons.item() / T_eval\n",
    "                        test_pred_full += l_pred.item()\n",
    "                        \n",
    "                        loc = loc + 1\n",
    "                    mse_error = mse_error / loc\n",
    "                test_recons_full = test_recons_full / len(evalloader)\n",
    "                test_pred_full = test_pred_full / len(evalloader)\n",
    "            self.scheduler.step()\n",
    "\n",
    "            if evalloader:\n",
    "                if ep == 0:\n",
    "                    print(\"Epoch\",\"Time\",\"[Train Recons MSE]\",\"[Train Pred MSE]\",\"[Eval Recons MSE]\",\"[Eval Pred MSE]\")\n",
    "                print(ep, t2 - t1, train_recons_full, train_pred_full, test_recons_full, test_pred_full)\n",
    "            else:\n",
    "                if ep == 0:\n",
    "                    print(\"Epoch\",\"Time\",\"Train Recons MSE\",\"Train Pred MSE\")\n",
    "                print(ep, t2 - t1, train_recons_full, train_pred_full)\n",
    "    def test(self, testloader, step = 1, T_out = 80, path = False, is_save = True, is_plot = False):\n",
    "        time_error = torch.zeros([T_out,1])\n",
    "        test_pred_full = 0\n",
    "        test_recons_full = 0\n",
    "        loc = 0\n",
    "        with torch.no_grad():\n",
    "            for xx, yy in testloader:\n",
    "                loss = 0\n",
    "                bs = xx.shape[0]\n",
    "                xx = xx.to(self.device)\n",
    "                yy = yy.to(self.device)\n",
    "                l_recons = 0\n",
    "                for t in range(0, T_out):\n",
    "                    y = yy[..., t:t + 1]\n",
    "                    im, im_re = self.kernel(xx)\n",
    "                    l_recons += self.loss(im_re.reshape(bs, -1), xx.reshape(bs, -1))\n",
    "                    t_error = self.loss(im[...,-1:],y)\n",
    "                    if t == 0:\n",
    "                        pred = im[...,-1:]\n",
    "                    else:\n",
    "                        pred = torch.cat((pred, im[...,-1:]), -1)\n",
    "                    time_error[t] = time_error[t] + t_error.item()\n",
    "                    xx = torch.cat((xx[..., 1:], im[...,-1:]), dim=-1)\n",
    "\n",
    "                test_recons_full += l_recons.item() / T_out\n",
    "                l_pred = self.loss(pred.reshape(bs, -1), yy.reshape(bs, -1))\n",
    "                test_pred_full += l_pred.item()\n",
    "                if(is_save):\n",
    "                    torch.save({\"pred\": pred, \"yy\": yy}, f\"{path}pred_yy_loc{loc}.pt\")\n",
    "                \n",
    "                if(is_plot):\n",
    "                    for i in range(T_out):\n",
    "                        plt.subplot(1,3,1)\n",
    "                        plt.title(\"Predict\")\n",
    "                        plt.imshow(pred[0,...,i].cpu().detach().numpy())\n",
    "                        plt.subplot(1,3,2)\n",
    "                        plt.imshow(yy[0,...,i].cpu().detach().numpy())\n",
    "                        plt.title(\"Label\")\n",
    "                        plt.subplot(1,3,3)\n",
    "                        plt.imshow(pred[0,...,i].cpu().detach().numpy()-yy[0,...,i].cpu().detach().numpy())\n",
    "                        plt.title(\"Error\")\n",
    "                        plt.show()\n",
    "                        plt.savefig(f\"{path}time_{i}_loc{loc}.png\")\n",
    "                        plt.close()\n",
    "                loc = loc + 1\n",
    "        test_pred_full = test_pred_full / loc\n",
    "        test_recons_full = test_recons_full / loc\n",
    "        time_error = time_error / len(testloader)\n",
    "        print(\"Total prediction test mse error is \",test_pred_full)\n",
    "        print(\"Total reconstruction test mse error is \",test_recons_full)\n",
    "        return time_error\n",
    "        \n",
    "    def save(self, path):\n",
    "        (fpath,_) = os.path.split(path)\n",
    "        if not os.path.isfile(fpath):\n",
    "            os.makedirs(fpath)\n",
    "        torch.save({\"koopman\":self,\"model\":self.kernel,\"model_params\":self.kernel.state_dict()}, path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./tutorial/NavierStokes_V1e-3_N5000_T50-013/ns_V1e-3_N5000_T50.mat\"\n",
    "train_loader, eval_loader = navier_stokes(data_path, batch_size = 1, T_in = 10, T_out = 40, type = \"1e-3\", sub = 1, reshape= True)\n",
    "\n",
    "# イテレータを使って最初のバッチを取得\n",
    "dataiter = iter(train_loader)\n",
    "x, y = next(dataiter)\n",
    "\n",
    "# データの形状を確認\n",
    "print(\"Input x shape:\", x.shape)\n",
    "print(\"Target y shape:\", y.shape)\n",
    "\n",
    "# データの値の範囲も確認\n",
    "print(\"\\nInput x stats:\")\n",
    "print(\"Min value:\", x.min().item())\n",
    "print(\"Max value:\", x.max().item())\n",
    "print(\"Mean value:\", x.mean().item())\n",
    "\n",
    "# メモリ上の位置とデータ型も確認\n",
    "print(\"\\nDevice:\", x.device)\n",
    "print(\"Data type:\", x.dtype)\n",
    "\n",
    "# バッチの1枚目の画像のチャンネルごとの統計も確認\n",
    "print(\"\\nFirst image in batch stats:\")\n",
    "print(\"Channel-wise mean:\", x[0].mean(dim=(1,2)))\n",
    "print(\"Channel-wise std:\", x[0].std(dim=(1,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameter definitions:\n",
    "device = torch.device(\"cuda\")\n",
    "# Hyper parameters\n",
    "ep = 1 # Training Epoch\n",
    "o = 32 # Koopman Operator Size\n",
    "m = 16 # Modes\n",
    "r = 8 # Power of Koopman Matrix\n",
    "\n",
    "# class koopman_vit:\n",
    "#     def __init__(self, decoder = \"Conv2d\", depth = 16, resolution=(256, 256), patch_size=(4, 4),\n",
    "#             in_chans=1, out_chans=1, embed_dim=768, parallel = False, device = False):\n",
    "ViT_KNO = koopman_vit(decoder = \"Conv2d\", resolution=(64, 64), patch_size=(2, 2),\n",
    "            in_chans=10, out_chans=10, embed_dim=768, depth = 16, parallel = True, device=device)\n",
    "ViT_KNO.compile()\n",
    "T_out = 40\n",
    "# ViT_KNO.train_single(epochs=ep, trainloader = train_loader, evalloader = eval_loader)\n",
    "# ViT_KNO.test_single(test_loader)\n",
    "ViT_KNO.train_multi(epochs=ep, trainloader = train_loader, evalloader = eval_loader, T_out = T_out)\n",
    "ViT_KNO.test_multi(test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBASデータローダ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "def resize_with_cv2(disp_arr, target_size=(64, 64)):\n",
    "    \"\"\"OpenCVを使ってリサイズ\"\"\"\n",
    "    h, w, c = disp_arr.shape\n",
    "    resized_channels = [cv2.resize(disp_arr[:, :, i], target_size, interpolation=cv2.INTER_LINEAR) for i in range(c)]\n",
    "    return np.stack(resized_channels, axis=-1)\n",
    "\n",
    "\n",
    "# SBASデータのロード\n",
    "def load_disp_data(parent_dir, target_size=(64, 64), target_channels=100):\n",
    "    \"\"\"フォルダから変位データを読み込み、128×128にリサイズ\"\"\"\n",
    "    disp_data = []\n",
    "    \n",
    "    for subfolder in sorted(os.listdir(parent_dir)):\n",
    "        subfolder_path = os.path.join(parent_dir, subfolder)\n",
    "        \n",
    "        if not os.path.isdir(subfolder_path):\n",
    "            continue\n",
    "            \n",
    "        h5_file_path = os.path.join(subfolder_path, \"cum_filt.h5\")\n",
    "        \n",
    "        if not os.path.exists(h5_file_path):\n",
    "            print(f\"File not found: {h5_file_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Loading: {h5_file_path}\")\n",
    "        with h5py.File(h5_file_path, \"r\") as h5_file:\n",
    "            disp_arr_chw = h5_file['cum']\n",
    "            disp_arr = np.transpose(disp_arr_chw, (1, 2, 0))  # (H, W, C)\n",
    "            \n",
    "            h, w, c = disp_arr.shape\n",
    "            print(f\"Shape of disp_arr: {h} x {w} x {c}\")\n",
    "            \n",
    "            # チャンネル数が100未満の場合はスキップ\n",
    "            if c < target_channels:\n",
    "                print(f\"Skipping {subfolder} (Channels: {c} < {target_channels})\")\n",
    "                continue\n",
    "            \n",
    "            # チャンネル数を100に制限\n",
    "            disp_arr = disp_arr[:, :, :target_channels]\n",
    "            \n",
    "            # NaNを0に置き換え\n",
    "            disp_arr = np.nan_to_num(disp_arr, nan=0.0)\n",
    "            \n",
    "            # **リサイズ (H, W) → (128, 128)**\n",
    "            disp_resized = resize_with_cv2(disp_arr)  # 線形補間\n",
    "            \n",
    "            print(f\"Resized shape: {disp_resized.shape}\")\n",
    "            disp_data.append(disp_resized)\n",
    "\n",
    "    \n",
    "    \n",
    "    return disp_data\n",
    "\n",
    "parent_dir = \"E:/2024/koopman/sbas/result\"\n",
    "disp_data = load_disp_data(parent_dir)\n",
    "disp_data = np.array(disp_data)\n",
    "# 予測する時間を定義\n",
    "T_in=20 # インプットフレーム数\n",
    "T_out=40 # 予測フレーム数\n",
    "batch_size =4\n",
    "# Traning data\n",
    "# 最初のT_inフレームを入力\n",
    "train_a = disp_data[:,:,:,:T_in]\n",
    "# 最初のT_inからT_outフレームを予測\n",
    "train_u = disp_data[:,:,:,T_in:T_out+T_in]\n",
    "# Testing data\n",
    "test_a = train_a\n",
    "test_u = disp_data[:,:,:,T_in:T_out+T_in+T_out]\n",
    "print(\"Shallow Water Equations Dataset has been loaded successfully!\")\n",
    "print(\"X train shape:\", train_a.shape, \"Y train shape:\", train_u.shape)\n",
    "print(\"X test shape:\", test_a.shape, \"Y test shape:\", test_u.shape)\n",
    "\n",
    "train_a = torch.from_numpy(train_a).float()  # float型に変換\n",
    "train_u = torch.from_numpy(train_u).float()\n",
    "test_a = torch.from_numpy(test_a).float()\n",
    "test_u = torch.from_numpy(test_u).float()\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_a, train_u), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(test_a, test_u), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# データローダーからiteratorを取得\n",
    "data_iter = iter(train_loader)\n",
    "# 最初の1つを取得\n",
    "batch = next(data_iter)\n",
    "for tmp in batch:\n",
    "    # [入力,予測対象]のpytorch配列のリスト\n",
    "    # torch.Size([1, 64, 64, 10])\n",
    "    # torch.Size([1, 64, 64, 40])\n",
    "    # バッチサイズｘ縦ｘ横ｘチャンネルの次元\n",
    "    print(tmp.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNベースの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import h5py\n",
    "    \n",
    "# Setting your computing device\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig_path = \"./demo/fig/\"\n",
    "save_path = \"./demo/result/\"\n",
    "os.makedirs(fig_path, exist_ok=True)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Hyper parameters\n",
    "ep = 1000 # Training Epoch\n",
    "o = 32 # Koopman Operator Size\n",
    "m = 16 # Modes\n",
    "r = 8 # Power of Koopman Matrix\n",
    "\n",
    "# Model\n",
    "koopman_model = koopman(backbone = \"KNO2d\", autoencoder = \"Conv2d\", o = o, m = m, r = r, t_in = 20, device = device)\n",
    "koopman_model.compile()\n",
    "koopman_model.opt_init(\"Adam\", lr = 0.005, step_size=250, gamma=0.5)\n",
    "koopman_model.train(epochs=ep, trainloader = train_loader, evalloader = test_loader)\n",
    "\n",
    "# Result and Saving\n",
    "time_error = koopman_model.test(test_loader, path = fig_path, is_save = True, is_plot = True)\n",
    "filename = \"koopmanAE\" + str(o) + \"m\" + str(m) + \"r\" +str(r) + \".pt\"\n",
    "torch.save({\"time_error\":time_error,\"params\":koopman_model.params}, save_path + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ファイルをロード\n",
    "file_path = \"./demo/fig/pred_yy_loc2.pt\"\n",
    "\n",
    "# データをロード\n",
    "data = torch.load(file_path)\n",
    "pred = data[\"pred\"].cpu().numpy()  # 形状: [4, 64, 64, 80]\n",
    "yy = data[\"yy\"].cpu().numpy()      # 形状: [4, 64, 64, 80]\n",
    "\n",
    "# 保存ディレクトリ作成\n",
    "save_dir = \"./demo/pred_yy_images/pred_yy_loc2\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 全データの最小値と最大値を取得（スケール統一）\n",
    "diff_arr = pred - yy \n",
    "global_min_diff = diff_arr.min()\n",
    "global_max_diff = diff_arr.max()\n",
    "\n",
    "global_min = min(pred.min(), yy.min())\n",
    "global_max = max(pred.max(), yy.max())\n",
    "\n",
    "# バッチ・チャンネルごとに画像を保存\n",
    "for batch_idx in range(pred.shape[0]):\n",
    "    for channel in range(pred.shape[3]):  # 時間チャンネルごとに処理\n",
    "        # 各画像を取得\n",
    "        img_pred = pred[batch_idx, :, :, channel]\n",
    "        img_yy = yy[batch_idx, :, :, channel]\n",
    "        img_diff = img_pred - img_yy  # 差分画像\n",
    "\n",
    "        # すべての画像を同じスケールで正規化\n",
    "        img_pred_norm = (img_pred - global_min) / (global_max - global_min)\n",
    "        img_yy_norm = (img_yy - global_min) / (global_max - global_min)\n",
    "        img_diff_norm = (img_diff - global_min_diff) / (global_max_diff - global_min_diff)  # 差分画像は独立スケール\n",
    "\n",
    "        # カラーマップ適用（rainbow）\n",
    "        img_pred_colored = plt.cm.rainbow(np.clip(img_pred_norm, 0, 1))\n",
    "        img_yy_colored = plt.cm.rainbow(np.clip(img_yy_norm, 0, 1))\n",
    "        img_diff_colored = plt.cm.rainbow(np.clip(img_diff_norm, 0, 1))\n",
    "\n",
    "        # 横に結合して1枚の画像に\n",
    "        combined_img = np.hstack([img_pred_colored, img_yy_colored, img_diff_colored])\n",
    "\n",
    "        # 画像保存\n",
    "        img_filename = f\"{save_dir}/batch{batch_idx}_channel{channel}.png\"\n",
    "        plt.imsave(img_filename, combined_img)\n",
    "\n",
    "print(\"画像の保存が完了しました！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ほかの手法との比較"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoEncoder+動的モード分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.linalg as linalg\n",
    "\n",
    "class DMD_Operator2D(nn.Module):\n",
    "    def __init__(self, op_size):\n",
    "        super(DMD_Operator2D, self).__init__()\n",
    "        self.op_size = op_size\n",
    "        self.A = None  # DMD行列\n",
    "\n",
    "    def compute_dmd_matrix(self, X, Y):\n",
    "        \"\"\" DMD 行列 A を計算 \"\"\"\n",
    "        U, S, Vh = linalg.svd(X, full_matrices=False)\n",
    "        S_inv = torch.diag(1.0 / S)\n",
    "        self.A = Y @ Vh.T @ S_inv @ U.T\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x_flat = x.view(B, C, -1)  # (B, C, H*W)\n",
    "        if self.A is None:\n",
    "            self.A = torch.eye(x_flat.shape[1], device=x.device)  # 初期状態\n",
    "        x_next = torch.matmul(self.A, x_flat)\n",
    "        return x_next.view(B, C, H, W)  # 元の形状に戻す\n",
    "\n",
    "class KNO2d_DMD(nn.Module):\n",
    "    def __init__(self, encoder, decoder, op_size, decompose=6, linear_type=True, normalization=False):\n",
    "        super(KNO2d_DMD, self).__init__()\n",
    "        self.op_size = op_size\n",
    "        self.decompose = decompose\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "        self.dmd_layer = DMD_Operator2D(self.op_size)\n",
    "        self.w0 = nn.Conv2d(op_size, op_size, 1)\n",
    "        self.linear_type = linear_type\n",
    "        self.normalization = normalization\n",
    "        if self.normalization:\n",
    "            self.norm_layer = nn.BatchNorm2d(op_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reconstruct\n",
    "        x_reconstruct = self.enc(x)\n",
    "        x_reconstruct = torch.tanh(x_reconstruct)\n",
    "        x_reconstruct = self.dec(x_reconstruct)\n",
    "        \n",
    "        # Predict\n",
    "        x = self.enc(x)  # Encoder\n",
    "        x = torch.tanh(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x_w = x\n",
    "        for i in range(self.decompose):\n",
    "            x1 = self.dmd_layer(x)  # DMD Operator\n",
    "            if self.linear_type:\n",
    "                x = x + x1\n",
    "            else:\n",
    "                x = torch.tanh(x + x1)\n",
    "        if self.normalization:\n",
    "            x = torch.tanh(self.norm_layer(self.w0(x_w)) + x)\n",
    "        else:\n",
    "            x = torch.tanh(self.w0(x_w) + x)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.dec(x)  # Decoder\n",
    "        return x, x_reconstruct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import h5py\n",
    "    \n",
    "# Setting your computing device\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig_path = \"./demo/fig/\"\n",
    "save_path = \"./demo/result/\"\n",
    "os.makedirs(fig_path, exist_ok=True)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Hyper parameters\n",
    "ep = 1000 # Training Epoch\n",
    "o = 32 # Koopman Operator Size\n",
    "m = 16 # Modes\n",
    "r = 8 # Power of Koopman Matrix\n",
    "\n",
    "# Model\n",
    "koopman_model = koopman(backbone = \"DMD\", autoencoder = \"Conv2d\", o = o, m = m, r = r, t_in = 20, device = device)\n",
    "koopman_model.compile()\n",
    "koopman_model.opt_init(\"Adam\", lr = 0.005, step_size=250, gamma=0.5)\n",
    "koopman_model.train(epochs=ep, trainloader = train_loader, evalloader = test_loader)\n",
    "\n",
    "# Result and Saving\n",
    "time_error = koopman_model.test(test_loader, path = fig_path, is_save = True, is_plot = True)\n",
    "filename = \"ns_time_error_op\" + str(o) + \"m\" + str(m) + \"r\" +str(r) + \".pt\"\n",
    "torch.save({\"time_error\":time_error,\"params\":koopman_model.params}, save_path + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoEncoder+Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateSpace2D(nn.Module):\n",
    "    def __init__(self, hidden_size, state_size):\n",
    "        super(StateSpace2D, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # State space model parameters\n",
    "        self.A = nn.Parameter(torch.randn(hidden_size, hidden_size))  # State transition matrix\n",
    "        self.B = nn.Parameter(torch.randn(hidden_size, state_size))   # Input matrix\n",
    "        self.C = nn.Parameter(torch.randn(state_size, hidden_size))   # Output matrix\n",
    "        self.D = nn.Parameter(torch.randn(state_size, state_size))    # Direct transmission matrix\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, height, width, channels = x.shape\n",
    "        # Reshape input for state space processing\n",
    "        x_flat = x.reshape(batch_size, height * width, channels)\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h = torch.zeros(batch_size, height * width, self.hidden_size, device=x.device)\n",
    "        \n",
    "        # State space update\n",
    "        h_next = torch.bmm(h, self.A.expand(batch_size, -1, -1)) + \\\n",
    "                 torch.bmm(x_flat, self.B.expand(batch_size, -1, -1))\n",
    "        \n",
    "        # Output equation\n",
    "        y = torch.bmm(h_next, self.C.transpose(0, 1).expand(batch_size, -1, -1)) + \\\n",
    "            torch.bmm(x_flat, self.D.expand(batch_size, -1, -1))\n",
    "            \n",
    "        # Reshape output back to original dimensions\n",
    "        y = y.reshape(batch_size, height, width, channels)\n",
    "        \n",
    "        return y\n",
    "\n",
    "class SSM2d(nn.Module):\n",
    "    def __init__(self, encoder, decoder, op_size, decompose=6, \n",
    "                 linear_type=True, normalization=False, hidden_size=64):\n",
    "        super(SSM2d, self).__init__()\n",
    "        # Parameter\n",
    "        self.op_size = op_size\n",
    "        self.decompose = decompose\n",
    "        \n",
    "        # Layer Structure\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "        self.state_space = StateSpace2D(hidden_size=hidden_size, state_size=op_size)\n",
    "        self.w0 = nn.Conv2d(op_size, op_size, 1)\n",
    "        self.linear_type = linear_type\n",
    "        self.normalization = normalization\n",
    "        if self.normalization:\n",
    "            self.norm_layer = torch.nn.BatchNorm2d(op_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reconstruct\n",
    "        x_reconstruct = self.enc(x)\n",
    "        x_reconstruct = torch.tanh(x_reconstruct)\n",
    "        x_reconstruct = self.dec(x_reconstruct)\n",
    "        \n",
    "        # Predict\n",
    "        x = self.enc(x)  # Encoder\n",
    "        x = torch.tanh(x)\n",
    "        x_w = x.permute(0, 3, 1, 2)\n",
    "        x = x\n",
    "        \n",
    "        # Apply state space model iteratively\n",
    "        for i in range(self.decompose):\n",
    "            x1 = self.state_space(x)  # State Space Model\n",
    "            if self.linear_type:\n",
    "                x = x + x1\n",
    "            else:\n",
    "                x = torch.tanh(x + x1)\n",
    "        \n",
    "        if self.normalization:\n",
    "            x = torch.tanh(self.norm_layer(self.w0(x_w)) + x.permute(0, 3, 1, 2))\n",
    "        else:\n",
    "            x = torch.tanh(self.w0(x_w) + x.permute(0, 3, 1, 2))\n",
    "            \n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.dec(x)  # Decoder\n",
    "        return x, x_reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import h5py\n",
    "    \n",
    "# Setting your computing device\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "fig_path = \"./demo/fig/\"\n",
    "save_path = \"./demo/result/\"\n",
    "os.makedirs(fig_path, exist_ok=True)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Hyper parameters\n",
    "ep = 1000 # Training Epoch\n",
    "o = 32 # Koopman Operator Size\n",
    "m = 16 # Modes\n",
    "r = 8 # Power of Koopman Matrix\n",
    "\n",
    "# Model\n",
    "koopman_model = koopman(backbone = \"SSM\", autoencoder = \"Conv2d\", o = o, m = m, r = r, t_in = 20, device = device)\n",
    "koopman_model.compile()\n",
    "koopman_model.opt_init(\"Adam\", lr = 0.005, step_size=250, gamma=0.5)\n",
    "koopman_model.train(epochs=ep, trainloader = train_loader, evalloader = test_loader)\n",
    "\n",
    "# Result and Saving\n",
    "time_error = koopman_model.test(test_loader, path = fig_path, is_save = True, is_plot = True)\n",
    "filename = \"ns_time_error_op\" + str(o) + \"m\" + str(m) + \"r\" +str(r) + \".pt\"\n",
    "torch.save({\"time_error\":time_error,\"params\":koopman_model.params}, save_path + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoEncoder無しのDMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from numpy import dot, multiply, diag, power\n",
    "from numpy import pi, exp, sin, cos, cosh, tanh, real, imag\n",
    "from numpy.linalg import inv, eig, pinv\n",
    "from scipy.linalg import svd, svdvals\n",
    "from scipy.integrate import odeint, ode, complex_ode\n",
    "from warnings import warn\n",
    "from scipy.linalg import svd\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "from scipy.interpolate import griddata\n",
    "import scipy.integrate\n",
    "\n",
    "from matplotlib import animation\n",
    "from matplotlib import pyplot as plt\n",
    "from pydmd import DMD\n",
    "from pydmd.plotter import plot_modes_2D\n",
    "from scipy import interpolate\n",
    "def resize_with_cv2(disp_arr, target_size=(64, 64)):\n",
    "    \"\"\"OpenCVを使ってリサイズ\"\"\"\n",
    "    h, w, c = disp_arr.shape\n",
    "    resized_channels = [cv2.resize(disp_arr[:, :, i], target_size, interpolation=cv2.INTER_LINEAR) for i in range(c)]\n",
    "    return np.stack(resized_channels, axis=-1)\n",
    "def create_animation(arr, title='Time Series Animation', cmap='viridis', interval=200):\n",
    "    \"\"\"\n",
    "    3D NumPy配列をアニメーションとして可視化\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    arr : numpy.ndarray\n",
    "        3D配列 (縦  横 時間 )\n",
    "    title : str, optional\n",
    "        アニメーションのタイトル\n",
    "    cmap : str, optional\n",
    "        カラーマップ\n",
    "    interval : int, optional\n",
    "        フレーム間隔（ミリ秒）\n",
    "    \"\"\"\n",
    "    # フィギュアとアクシスを作成\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # カラーマップの範囲を固定\n",
    "    vmin = np.nanmin(arr)\n",
    "    vmax = np.nanmax(arr)\n",
    "    \n",
    "    # 最初のフレームを表示\n",
    "    im = ax.imshow(arr[:,:,0], cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar(im, ax=ax, label='Value')\n",
    "    \n",
    "    # タイトルと軸ラベル\n",
    "    ax.set_title(f'{title} - Frame 0')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    \n",
    "    # アニメーション更新関数\n",
    "    def update(frame):\n",
    "        im.set_array(arr[:,:,frame])\n",
    "        ax.set_title(f'{title} - Frame {frame}')\n",
    "        return [im]\n",
    "    \n",
    "    # アニメーションを作成\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, \n",
    "        update, \n",
    "        frames=arr.shape[2],  # 時間軸の長さ\n",
    "        interval=interval,    # フレーム間隔\n",
    "        blit=True             # パフォーマンス最適化\n",
    "    )\n",
    "    \n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from numpy import dot, multiply, diag, power\n",
    "from numpy import pi, exp, sin, cos, cosh, tanh, real, imag\n",
    "from numpy.linalg import inv, eig, pinv\n",
    "from scipy.linalg import svd, svdvals\n",
    "from scipy.integrate import odeint, ode, complex_ode\n",
    "from warnings import warn\n",
    "from scipy.linalg import svd\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "from scipy.interpolate import griddata\n",
    "import scipy.integrate\n",
    "\n",
    "from matplotlib import animation\n",
    "from pydmd import DMD\n",
    "from pydmd.plotter import plot_modes_2D\n",
    "from scipy import interpolate\n",
    "import torch\n",
    "from scipy import linalg\n",
    "\n",
    "def calculate_r2_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    NaNを考慮したR2スコアを計算する関数\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : numpy.ndarray\n",
    "        真値\n",
    "    y_pred : numpy.ndarray\n",
    "        予測値\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        R2スコア\n",
    "    \"\"\"\n",
    "    # NaNを除外してマスクを作成\n",
    "    mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
    "    \n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "    \n",
    "    if len(y_true) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # R2スコアの計算\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    \n",
    "    if ss_tot == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "def calculate_metrics(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    予測結果と真値の精度評価を行う関数\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : numpy.ndarray\n",
    "        予測結果\n",
    "    ground_truth : numpy.ndarray\n",
    "        真値\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        各種評価指標\n",
    "    \"\"\"\n",
    "    # データを2次元に変形 (N, T, H, W) -> (N*H*W, T)\n",
    "    pred_flat = predictions.reshape(-1, predictions.shape[-1])\n",
    "    true_flat = ground_truth.reshape(-1, ground_truth.shape[-1])\n",
    "    \n",
    "    # NaNを除外したマスク\n",
    "    mask = ~(np.isnan(pred_flat) | np.isnan(true_flat))\n",
    "    \n",
    "    # 有効なデータの割合を計算\n",
    "    valid_ratio = np.mean(mask) * 100\n",
    "    \n",
    "    # マスクを適用してNaNを除外\n",
    "    pred_valid = pred_flat[mask]\n",
    "    true_valid = true_flat[mask]\n",
    "    \n",
    "    if len(true_valid) == 0:\n",
    "        return {\n",
    "            'MSE': np.nan,\n",
    "            'RMSE': np.nan,\n",
    "            'MAE': np.nan,\n",
    "            'R2': np.nan,\n",
    "            'Relative_Error': np.nan,\n",
    "            'Max_Error': np.nan,\n",
    "            'Valid_Data_Ratio': 0.0\n",
    "        }\n",
    "    \n",
    "    # 各種指標の計算\n",
    "    mse = np.mean((pred_valid - true_valid) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(pred_valid - true_valid))\n",
    "    r2 = calculate_r2_score(true_valid, pred_valid)\n",
    "    rel_error = np.mean(np.abs(pred_valid - true_valid) / (np.abs(true_valid) + 1e-8))\n",
    "    max_error = np.max(np.abs(pred_valid - true_valid))\n",
    "    \n",
    "    metrics = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'Relative_Error': rel_error,\n",
    "        'Max_Error': max_error,\n",
    "        'Valid_Data_Ratio': valid_ratio\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_temporal_metrics(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    時間方向の評価指標を計算する関数\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : numpy.ndarray\n",
    "        予測結果\n",
    "    ground_truth : numpy.ndarray\n",
    "        真値\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        時間方向の評価指標\n",
    "    \"\"\"\n",
    "    # 時間方向のMSEを計算\n",
    "    temporal_mse = np.zeros(predictions.shape[-1])\n",
    "    temporal_mae = np.zeros(predictions.shape[-1])\n",
    "    temporal_r2 = np.zeros(predictions.shape[-1])\n",
    "    \n",
    "    for t in range(predictions.shape[-1]):\n",
    "        # 各時刻でのデータ\n",
    "        pred_t = predictions[..., t].flatten()\n",
    "        true_t = ground_truth[..., t].flatten()\n",
    "        \n",
    "        # NaNを除外\n",
    "        mask = ~(np.isnan(pred_t) | np.isnan(true_t))\n",
    "        pred_valid = pred_t[mask]\n",
    "        true_valid = true_t[mask]\n",
    "        \n",
    "        if len(true_valid) > 0:\n",
    "            temporal_mse[t] = np.mean((pred_valid - true_valid) ** 2)\n",
    "            temporal_mae[t] = np.mean(np.abs(pred_valid - true_valid))\n",
    "            temporal_r2[t] = calculate_r2_score(true_valid, pred_valid)\n",
    "    \n",
    "    return {\n",
    "        'temporal_mse': temporal_mse,\n",
    "        'temporal_mae': temporal_mae,\n",
    "        'temporal_r2': temporal_r2\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# File path\n",
    "parent_dir = \"E:/2024/koopman/sbas/result\"\n",
    "\"\"\"フォルダから変位データを読み込み、128×128にリサイズ\"\"\"\n",
    "pred_data = []\n",
    "gt_data = []\n",
    "\n",
    "for subfolder in sorted(os.listdir(parent_dir)):\n",
    "    subfolder_path = os.path.join(parent_dir, subfolder)\n",
    "    \n",
    "    if not os.path.isdir(subfolder_path):\n",
    "        continue\n",
    "        \n",
    "    h5_file_path = os.path.join(subfolder_path, \"cum_filt.h5\")\n",
    "    \n",
    "    if not os.path.exists(h5_file_path):\n",
    "        print(f\"File not found: {h5_file_path}\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Loading: {h5_file_path}\")\n",
    "    with h5py.File(h5_file_path, \"r\") as cumh5:\n",
    "        disp_arr_chw = cumh5['cum']\n",
    "        disp_arr = np.transpose(disp_arr_chw, (1, 2, 0))        # C, H, W => H,W,C\n",
    "        if(disp_arr.shape[2]<100):continue\n",
    "        vel = cumh5['vel']\n",
    "\n",
    "        nanmean_time = np.nanmean(disp_arr, axis=2)\n",
    "        disp_arr = np.nan_to_num(disp_arr, nan=0.0)  # NaN をゼロに置き換える\n",
    "        disp_arr = resize_with_cv2(disp_arr)  # 線形補間\n",
    "        h = disp_arr.shape[0]\n",
    "        w = disp_arr.shape[1]\n",
    "        c = 20\n",
    "    \n",
    "        inputdata = disp_arr[:,:,:20]\n",
    "        gtdata = disp_arr[:,:,20:100]\n",
    "        \n",
    "        # DMD\n",
    "        dmd = DMD(svd_rank=1, tlsq_rank=2, exact=True, opt=True)\n",
    "\n",
    "        dmd.fit(inputdata)\n",
    "    \n",
    "        #　きちんと復元できているかを確認する\n",
    "        arr_3d = dmd.reconstructed_data.reshape(h, w, c).real\n",
    "        print(\"Shape arr_3d: {}\".format(arr_3d.shape))\n",
    "\n",
    "        #######################################\n",
    "        ############ ここから予測  ##############\n",
    "        #######################################\n",
    "        dmd.dmd_time[\"tend\"] += 80 # 300ステップ後まで将来予測\n",
    "\n",
    "        # 予測結果の配列を画像形式に戻す\n",
    "        arr_3d_pred = dmd.reconstructed_data.reshape(h, w, c + 80).real\n",
    "        arr_3d_pred_CHW = np.transpose(arr_3d_pred, (2, 0, 1))        # H, W, C => C,H,W\n",
    "        print(\"Shape after manipulation: {}\".format(arr_3d_pred.shape))\n",
    "        arr_3d_pred = np.nan_to_num(arr_3d_pred, nan=0.0)  # NaN をゼロに置き換える\n",
    "        pred_data.append(arr_3d_pred)\n",
    "        gt_data.append(gtdata)\n",
    "\n",
    "    \n",
    "all_predictions_u = np.array(pred_data)\n",
    "all_predictions_u = all_predictions_u[:,:,:,20:]\n",
    "all_ground_truth_u = np.array(gt_data)\n",
    "\n",
    "# 精度評価\n",
    "metrics = calculate_metrics(all_predictions_u, all_ground_truth_u)\n",
    "temporal_metrics = calculate_temporal_metrics(all_predictions_u, all_ground_truth_u)\n",
    "# # 結果の表示\n",
    "print(\"\\nAccuracy Metrics:\")\n",
    "print(\"-\" * 40)\n",
    "for metric_name, value in metrics.items():\n",
    "    print(f\"{metric_name:15s}: {value:.6f}\")\n",
    "\n",
    "print(\"\\nTemporal Error Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Best timestep (MSE): {np.argmin(temporal_metrics['temporal_mse'])} \"\n",
    "      f\"(MSE: {np.min(temporal_metrics['temporal_mse']):.6f})\")\n",
    "print(f\"Worst timestep (MSE): {np.argmax(temporal_metrics['temporal_mse'])} \"\n",
    "      f\"(MSE: {np.max(temporal_metrics['temporal_mse']):.6f})\")\n",
    "print(f\"Average R2 score: {np.mean(temporal_metrics['temporal_r2']):.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoEncoder無しのSSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "# データの読み込みと前処理\n",
    "def load_and_preprocess_data(parent_dir):\n",
    "    input_data, gt_data = [], []\n",
    "    \n",
    "    for subfolder in sorted(os.listdir(parent_dir)):\n",
    "        subfolder_path = os.path.join(parent_dir, subfolder)\n",
    "        if not os.path.isdir(subfolder_path):\n",
    "            continue\n",
    "        \n",
    "        h5_file_path = os.path.join(subfolder_path, \"cum_filt.h5\")\n",
    "        \n",
    "        if not os.path.exists(h5_file_path):\n",
    "            print(f\"File not found: {h5_file_path}\")\n",
    "            continue\n",
    "        \n",
    "        with h5py.File(h5_file_path, \"r\") as cumh5:\n",
    "            disp_arr = np.transpose(cumh5['cum'], (1, 2, 0))  # C, H, W -> H, W, C\n",
    "            disp_arr = np.nan_to_num(disp_arr, nan=0.0)  # NaN をゼロに置き換える\n",
    "            disp_arr = resize_with_cv2(disp_arr)\n",
    "            if disp_arr.shape[2] < 100:\n",
    "                continue\n",
    "            \n",
    "            input_seq = disp_arr[:, :, :20]\n",
    "            gt_seq = disp_arr[:, :, 20:100]\n",
    "            \n",
    "            input_data.append(input_seq)\n",
    "            gt_data.append(gt_seq)\n",
    "    \n",
    "    return np.array(input_data), np.array(gt_data)\n",
    "\n",
    "# 状態空間モデルによる将来予測\n",
    "def predict_with_ssm(input_data, future_steps=80):\n",
    "    height, width, time_steps = input_data.shape\n",
    "    predictions = np.zeros((height, width, time_steps + future_steps))\n",
    "    predictions[:, :, :time_steps] = input_data\n",
    "    \n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            ts = input_data[i, j, :]\n",
    "            ts = np.nan_to_num(ts, nan=0.0)  # NaNをゼロに置き換え\n",
    "            \n",
    "            kf = KalmanFilter(\n",
    "                transition_matrices=[[1]],  # 状態遷移行列\n",
    "                observation_matrices=[[1]],  # 観測行列\n",
    "                initial_state_mean=ts[0],\n",
    "                n_dim_obs=1\n",
    "            )\n",
    "            kf = kf.em(ts, n_iter=10)  # EMアルゴリズムでパラメータ推定\n",
    "            \n",
    "            filtered_state_means, _ = kf.filter(ts)\n",
    "            future_state_means, _ = kf.smooth(np.concatenate([ts, np.zeros(future_steps)]))\n",
    "            \n",
    "            predictions[i, j, :] = future_state_means[:, 0]\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 精度評価\n",
    "def calculate_metrics(predictions, ground_truth):\n",
    "    mse = np.mean((predictions - ground_truth) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return {'MSE': mse, 'RMSE': rmse}\n",
    "\n",
    "# 実行\n",
    "data_dir = \"E:/2024/koopman/sbas/result\"\n",
    "input_data, ground_truth = load_and_preprocess_data(data_dir)\n",
    "predictions=[]\n",
    "for one_data in input_data:\n",
    "    # print()\n",
    "    predictions.append(predict_with_ssm(one_data))\n",
    "predictions=np.array(predictions)  \n",
    "metrics = calculate_metrics(predictions[:, :, 20:], ground_truth)\n",
    "\n",
    "print(\"Prediction Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoEncoder無しのkoopman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def resize_with_cv2(disp_arr, target_size=(64, 64)):\n",
    "    \"\"\"OpenCVを使ってリサイズ\"\"\"\n",
    "    h, w, c = disp_arr.shape\n",
    "    resized_channels = [cv2.resize(disp_arr[:, :, i], target_size, interpolation=cv2.INTER_LINEAR) for i in range(c)]\n",
    "    return np.stack(resized_channels, axis=-1)\n",
    "    \n",
    "# データの読み込みと前処理\n",
    "def load_and_preprocess_data(parent_dir):\n",
    "    input_data, gt_data = [], []\n",
    "    count=0\n",
    "    for subfolder in sorted(os.listdir(parent_dir)):\n",
    "        subfolder_path = os.path.join(parent_dir, subfolder)\n",
    "        if not os.path.isdir(subfolder_path):\n",
    "            continue\n",
    "        \n",
    "        h5_file_path = os.path.join(subfolder_path, \"cum_filt.h5\")\n",
    "        if not os.path.exists(h5_file_path):\n",
    "            print(f\"File not found: {h5_file_path}\")\n",
    "            continue\n",
    "        \n",
    "        with h5py.File(h5_file_path, \"r\") as cumh5:\n",
    "            disp_arr = np.transpose(cumh5['cum'], (1, 2, 0))  # C, H, W -> H, W, C\n",
    "            disp_arr = np.nan_to_num(disp_arr, nan=0.0)  # NaN をゼロに置き換える\n",
    "            disp_arr = resize_with_cv2(disp_arr)\n",
    "            \n",
    "            \n",
    "            if disp_arr.shape[2] < 100:\n",
    "                continue\n",
    "            \n",
    "            input_seq = disp_arr[:, :, :20]\n",
    "            gt_seq = disp_arr[:, :, 20:100]\n",
    "            \n",
    "            input_data.append(input_seq)\n",
    "            gt_data.append(gt_seq)\n",
    "            count+=1\n",
    "\n",
    "    return np.array(input_data), np.array(gt_data)\n",
    "\n",
    "# クープマン作用素を用いた将来予測\n",
    "def koopman_forecast(input_data, future_steps=80):\n",
    "    height, width, time_steps = input_data.shape\n",
    "    predictions = np.zeros((height, width, time_steps + future_steps))\n",
    "    predictions[:, :, :time_steps] = input_data\n",
    "    \n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            ts = input_data[i, j, :]\n",
    "            ts = np.nan_to_num(ts, nan=0.0)  # NaNをゼロに置き換え\n",
    "            \n",
    "            X = ts[:-1].reshape(-1, 1)  # 過去の状態\n",
    "            Y = ts[1:].reshape(-1, 1)  # 次の時刻の状態\n",
    "            \n",
    "            # クープマン作用素の近似として線形回帰を使用\n",
    "            K = np.linalg.pinv(X) @ Y  # ダイナミクス行列の推定\n",
    "            \n",
    "            # 未来の状態の予測\n",
    "            future_state = np.array([ts[-1]])  # 1D ベクトルとして扱う\n",
    "            for t in range(future_steps):\n",
    "                future_state = K @ future_state  # 行列積を適用\n",
    "                predictions[i, j, time_steps + t] = future_state[0]  # スカラー値に戻して格納\n",
    "    # print(predictions.shape)\n",
    "\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 精度評価\n",
    "def calculate_metrics(predictions, ground_truth):\n",
    "    mse = np.mean((predictions - ground_truth) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return {'MSE': mse, 'RMSE': rmse}\n",
    "\n",
    "# 実行\n",
    "data_dir = \"E:/2024/koopman/sbas/result\"\n",
    "input_data, ground_truth = load_and_preprocess_data(data_dir)\n",
    "predictions = []\n",
    "for one_data in input_data:\n",
    "    predictions.append(koopman_forecast(one_data))\n",
    "predictions = np.array(predictions)  \n",
    "metrics = calculate_metrics(predictions[:, :, :, 20:], ground_truth)\n",
    "\n",
    "print(\"Prediction Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
